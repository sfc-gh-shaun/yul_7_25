{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "4xhrhxkpfzhyi7ggu362",
   "authorId": "1449212902016",
   "authorName": "SODONNELL_SFC",
   "authorEmail": "shaun.odonnell@snowflake.com",
   "sessionId": "be701054-f610-40bd-96f3-586d64fe22c4",
   "lastEditTime": 1752693011214
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "cell1",
    "collapsed": false,
    "codeCollapsed": false
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e98bdf03-3bd8-4a48-bc64-186fb578f8f4",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "# Data Reporting and Machine Learning: Burn Rate Forecasting\n\n---\n\n## In-Scope Data Sources\n\n* **Costpoint**: budgets, actuals, labor data\n* **PDF / Excel files**: pricing & contract data\n* **HCSS**: labor data\n\n---\n\n## Functional Requirements\n\n### Data Integration\n\n* Build pipelines from Costpoint to extract budget, actual, and committed cost data.\n* Integrate labor data from Costpoint, Workday, HCSS, and Viewpoint for hours worked and labor categories.\n* Standardize data into a unified model (project ID, task, labor category, time period).\n* Add manual adjustments/forecast inputs from Excel, Word, and/or SharePoint if needed.\n\n### Data Transformations & Forecasting\n\n* Calculate **burn rate** (actual cost / total budget) * 100 over time.\n* Forecast future spend based on historical velocity and staffing plans.\n* Create margin projections based on the burn rate vs. revenue accruals.\n* Flag projects that exceed burn thresholds (e.g., ≥80% budget at 50% project completion).\n\n### Security & Governance\n\n* Apply **Role-Based Access Control (RBAC)**: project-level visibility for PMs; Finance can view all.\n* Mask or restrict access to individual wage data unless the user has an HR role.\n* Classify datasets (e.g., sensitive labor cost fields).\n* Track lineage from source system (i.e., Costpoint) through the data lake to the report.\n\n### Metadata & Documentation\n\n* Tag datasets as “financial-critical”, “forecasting”, “labor-sensitive”.\n* Add business glossary terms (e.g., burn-rate, earned value, EAC).\n* Assign a data steward to the forecasting dataset.\n* Document assumptions for all forecast logic in the catalog.\n\n### Reporting & Insights\n\n* Build dashboards for:\n    * Budget vs. Actual by month\n    * Forecast burn rate vs. baseline\n    * Heatmaps for over/under-spending by project\n    * Variance explanations (with auto-tagged reasons)\n* Export forecast summaries to Excel/CSV for program reviews.\n"
  },
  {
   "cell_type": "code",
   "id": "72e26693-db81-4445-89bb-a179d024d5f1",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "use database BURN_RATE_FORECAST;\nuse schema RAW;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "005c32bb-ce9e-43c2-a316-7c2df2916c22",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "### Upload the files to a stage in the above database.  You can use the UI (from the database navigator) or the command line interface SnowCLI."
  },
  {
   "cell_type": "code",
   "id": "c51559e8-5379-421c-b808-654de2dd6095",
   "metadata": {
    "language": "sql",
    "name": "cell4"
   },
   "outputs": [],
   "source": "list @RAWFILES;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ded6fea0-ca34-4f63-8caa-62c9363e2d7a",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "### These files can be queried directly, at least the json ones, or csv, parquet, etc.  PDFs we'll address in a moment."
  },
  {
   "cell_type": "code",
   "id": "fd51d310-d62d-4697-8cc3-aa5ebde853dd",
   "metadata": {
    "language": "sql",
    "name": "cell7"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE FILE FORMAT my_json_format\n  TYPE = 'JSON'\n  STRIP_OUTER_ARRAY = TRUE; -- Important: If your JSON file starts with an array, this treats each element of the array as a separate row.",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1099730d-2ae5-4a44-883a-3b4411236d9e",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "\n    SELECT\n    -- Accessing top-level elements:\n    PARSE_JSON($1):ActualCost::FLOAT AS ACTUAL_COST,\n    PARSE_JSON($1):BudgetedCost::VARCHAR AS BudgetedCost,\n    PARSE_JSON($1):ContractId::VARCHAR AS ContractId,\n    PARSE_JSON($1):CostCategory::VARCHAR AS CostCategory,\n    PARSE_JSON($1):Id::VARCHAR AS Id,\n    PARSE_JSON($1):Month::VARCHAR AS Month,\n    PARSE_JSON($1):Variance::VARCHAR AS Variance,\n  \n    -- You can also include metadata columns\n    METADATA$FILENAME AS source_filename,\n    METADATA$FILE_ROW_NUMBER AS row_in_file\nFROM\n    @BURN_RATE_FORECAST.RAW.RAWFILES/costpoint_burn_rate.json (FILE_FORMAT => ( 'my_json_format')) ;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f726ad02-c504-484e-8423-fd4cab9d543a",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "### That was unnecessarily difficult.  When we load the data, Snowflake can read the schema on injest and flatten automatically.  \n\n(Go to the UI and walk through loading a file and see the SQL) \n\nSo much simpler.  See below following the use of this process:"
  },
  {
   "cell_type": "code",
   "id": "aaf9898d-f4da-4122-bb83-a3835cc29623",
   "metadata": {
    "language": "sql",
    "name": "cell14"
   },
   "outputs": [],
   "source": "SELECT * FROM TABLE(INFER_SCHEMA(location=>'@\"BURN_RATE_FORECAST\".\"RAW\".\"RAWFILES\"', files=>('hcss_burn_rate.json'), file_format=>'my_json_format' , MAX_RECORDS_PER_FILE=>10000 ));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1bb835ef-f95f-4831-950c-cce402ca36cf",
   "metadata": {
    "language": "sql",
    "name": "cell5"
   },
   "outputs": [],
   "source": "select * from COSTPOINT_BURN_RATE limit 100;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ff972655-bb83-4ca2-a567-b1af790ee89e",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "### Naturally we aren't going to be uploading files one by one in a UI, so it is easy to build pipelines to handle this.  I'll use Python as an example - so useful that you can alternate between SQL and Python in the same notebook.  We can save these to the CORE schema in case they were already loaded."
  },
  {
   "cell_type": "code",
   "id": "1a434230-a8dd-48ff-9ccf-d4de5da83a3e",
   "metadata": {
    "language": "python",
    "name": "cell13"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col\nimport json # Used for pretty printing the inferred schema (optional)\n\ndef load_json_files_with_infer_schema(session: Session, stage_name: str, database_name: str, schema_name: str, dest_schema_name:str):\n    \"\"\"\n    Loops through JSON files in a specified internal stage, infers their schema,\n    creates tables with flattened columns based on the inferred schema,\n    and loads the JSON data into them.\n\n    Args:\n        session (Session): The Snowpark session object.\n        stage_name (str): The name of the internal stage (e.g., 'RAWFILES').\n        database_name (str): The name of the database where tables will be created.\n        schema_name (str): The name of the schema where tables will be created.\n    \"\"\"\n    print(f\"Starting JSON file loading process from stage: {stage_name} with INFER_SCHEMA\")\n\n    # Set the context for table creation\n    session.use_database(database_name)\n    session.use_schema(schema_name)\n\n    # List files in the stage\n    stage_files_df = session.sql(f\"LS @{stage_name}\").collect()\n\n    for file_info in stage_files_df:\n        file_name_with_path = file_info['name']\n        file_name_raw = file_name_with_path.split('/')[-1]\n        if not file_name_raw.lower().endswith('.json'):\n            print(f\"Skipping non-JSON file: {file_name_raw}\")\n            continue\n\n        table_name = file_name_raw.replace('.json', '_TST').upper()\n        file_path_on_stage = f\"@{stage_name}/{file_name_raw}\"\n\n        print(f\"\\nProcessing file: {file_name_raw}\")\n        print(f\"Target table: {table_name}\")\n\n        try:\n            # --- Check table existence using INFORMATION_SCHEMA ---\n            check_table_sql = f\"\"\"\n            SELECT COUNT(*)\n            FROM {database_name}.INFORMATION_SCHEMA.TABLES\n            WHERE TABLE_SCHEMA = '{schema_name.upper()}'\n              AND TABLE_NAME = '{table_name.upper()}'\n            ;\n            \"\"\"\n            table_exists_count = session.sql(check_table_sql).collect()[0][0]\n\n            if table_exists_count == 0:\n                print(f\"Table {table_name} does not exist. Inferring schema and creating table.\")\n\n                # 1. Infer Schema\n                # Use a temporary file format for schema inference\n                temp_file_format_name = f\"{table_name}_TEMP_FF\"\n                create_ff_sql = f\"\"\"\n                CREATE OR REPLACE FILE FORMAT {temp_file_format_name}\n                    TYPE = 'JSON'\n                    STRIP_OUTER_ARRAY = TRUE; -- Important: set to FALSE if top-level is an array of objects\n                \"\"\"\n                session.sql(create_ff_sql).collect()\n                print(f\"Created temporary file format: {temp_file_format_name}\")\n\n                infer_schema_sql = f\"\"\"\n                SELECT *\n                FROM TABLE(\n                    INFER_SCHEMA(\n                        LOCATION => '{file_path_on_stage}',\n                        FILE_FORMAT => '{temp_file_format_name}'\n                    )\n                )\n                \"\"\"\n                inferred_schema_df = session.sql(infer_schema_sql).collect()\n\n                # Build the CREATE TABLE DDL from the inferred schema\n                columns_ddl = []\n                for row in inferred_schema_df:\n                    column_name = row['COLUMN_NAME']\n                    # Use standard SQL types from inferred schema\n                    column_type = row['TYPE']\n                    # Adjust for potential unsupported types or add precision if needed\n                    # For example, if it infers DECIMAL(38,0) and you want INTEGER\n                    # Or if it infers a complex type that needs further handling\n                    columns_ddl.append(f\"{column_name} {column_type}\")\n\n                if not columns_ddl:\n                    print(f\"Warning: No columns inferred for {file_name_raw}. Skipping table creation.\")\n                    # Optionally, you might create a table with a single VARIANT column here\n                    continue\n\n                create_table_sql = f\"\"\"\n                CREATE OR REPLACE TABLE {database_name}.{dest_schema_name}.{table_name} (\n                    {', '.join(columns_ddl)}\n                );\n                \"\"\"\n                session.sql(create_table_sql).collect()\n                print(f\"Created table {table_name} with inferred schema:\")\n                for col_def in columns_ddl:\n                    print(f\"  - {col_def}\")\n\n                # Clean up temporary file format\n                session.sql(f\"DROP FILE FORMAT {temp_file_format_name}\").collect()\n                print(f\"Dropped temporary file format: {temp_file_format_name}\")\n\n            else:\n                print(f\"Table {table_name} already exists. Appending data (assuming schema matches).\")\n\n            # 2. Load Data using a SELECT from stage and table function for auto-flattening\n            # This uses the MATCH_RECOGNIZE pattern to map inferred columns to input file columns\n            # The 'TABLE(result_scan())' is a powerful way to reference the output of INFER_SCHEMA\n            # and then use it in the COPY INTO for projection.\n            # However, for direct COPY INTO with INFER_SCHEMA, a simpler approach is often needed\n            # based on the structure. If the INFER_SCHEMA works well, a direct COPY INTO\n            # FROM (SELECT $1:column_name::TYPE, ... FROM @stage) is more common.\n            #\n            # A more straightforward way to load into a flattened table after schema inference:\n            # COPY INTO <table_name> FROM (SELECT $1:<col1>::<type1>, $1:<col2>::<type2> ... FROM @stage)\n            # Snowflake's COPY INTO command with INFER_SCHEMA automatically handles mapping.\n\n            # The simplest way to load after INFER_SCHEMA has done its work:\n            copy_into_sql = f\"\"\"\n            COPY INTO {database_name}.{schema_name}.{table_name}\n            FROM @{stage_name}/{file_name_raw}\n            FILE_FORMAT = (TYPE = 'JSON')\n            MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE\n            ON_ERROR = 'CONTINUE';\n            \"\"\"\n            session.sql(copy_into_sql).collect()\n            print(f\"Successfully loaded {file_name_raw} into {table_name} with flattened data.\")\n\n        except Exception as e:\n            print(f\"Error processing {file_name_raw}: {e}\")\n            # Optionally, you might want to log this error to a table for review\n            # or move the problematic file to an error stage.\n\n# --- Example Usage in a Snowflake Notebook ---\ncurrent_session = session\n\n# --- Configuration ---\n# IMPORTANT: Replace these with your actual stage, database, and schema names\nmy_stage = 'RAWFILES' # e.g., 'RAW_JSON_STAGE'\nmy_database = 'BURN_RATE_FORECAST' # e.g., 'DEMO_DB'\nmy_schema = 'RAW'     # e.g., 'PUBLIC'\ndest_schema = 'CORE'\n\n# --- Run the function ---\nload_json_files_with_infer_schema(current_session, my_stage, my_database, my_schema, dest_schema)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e0be2087-c25c-4a9a-9631-2e14cc601451",
   "metadata": {
    "name": "cell15",
    "collapsed": false
   },
   "source": "### So what about the PDF files?  Several options, but let's jump out to DocumentAI and see how easy it is for a business user to create a model that can extract the salient info from a document like the contract.pdf"
  },
  {
   "cell_type": "code",
   "id": "5668dbed-aefb-4199-bfe8-3465f2d3828f",
   "metadata": {
    "language": "sql",
    "name": "cell11"
   },
   "outputs": [],
   "source": "--Here is how we call the model that 'predicts' the values we defined in DocumentAI\n\ncreate  table contract_data if not exists as \nSELECT BURN_RATE_FORECAST.RAW.CONTRACTS_EXTRACTION!PREDICT(\n  GET_PRESIGNED_URL(@RAWFILES, 'contract_document.pdf'), 1) JSON_DATA;\n\n\n--create table PDF_OCR_OUTPUT as\nSELECT\n    t.JSON_DATA:\"__documentMetadata\".ocrScore::FLOAT AS OCR_SCORE,\n    f_client.value:value::STRING AS CLIENT_VALUE,\n    f_client.value:score::FLOAT AS CLIENT_SCORE,\n    f_contract_ceiling.value:value::STRING AS CONTRACT_CEILING_VALUE,\n    f_contract_ceiling.value:score::FLOAT AS CONTRACT_CEILING_SCORE,\n    f_rate.value:value::STRING AS RATE_VALUE,\n    f_rate.value:score::FLOAT AS RATE_SCORE,\n    f_reporting_date.value:value::STRING AS REPORTING_DATE_VALUE,\n    f_reporting_date.value:score::FLOAT AS REPORTING_DATE_SCORE,\n    f_term_end.value:value::STRING AS TERM_END_VALUE,\n    f_term_end.value:score::FLOAT AS TERM_END_SCORE,\n    f_term_start.value:value::STRING AS TERM_START_VALUE,\n    f_term_start.value:score::FLOAT AS TERM_START_SCORE\nFROM\n    contract_data t,\n    LATERAL FLATTEN(input => t.JSON_DATA:client) f_client,\n    LATERAL FLATTEN(input => t.JSON_DATA:contract_ceiling) f_contract_ceiling,\n    LATERAL FLATTEN(input => t.JSON_DATA:rate) f_rate,\n    LATERAL FLATTEN(input => t.JSON_DATA:reporting_date) f_reporting_date,\n    LATERAL FLATTEN(input => t.JSON_DATA:term_end) f_term_end,\n    LATERAL FLATTEN(input => t.JSON_DATA:term_start) f_term_start;",
   "execution_count": null
  }
 ]
}